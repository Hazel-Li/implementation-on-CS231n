{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graident check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.layers import Conv, MaxPool, Linear, Relu\n",
    "from nn.cnn import CNN\n",
    "from check_gradient import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "check_conv = True\n",
    "check_pool_back = True\n",
    "check_conv_back = True\n",
    "check_cnn_back = True\n",
    "check_time = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward\n",
      "difference:  2.2121476417505994e-08\n"
     ]
    }
   ],
   "source": [
    "if check_conv:\n",
    "\n",
    "    x_shape = (2, 3, 4, 4)\n",
    "    w_shape = (3, 3, 4, 4)\n",
    "    x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "    w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "    b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "    conv = Conv(3, 3, 4, 4, 2, 1)\n",
    "    conv.params['w']['param'] = w\n",
    "    conv.params['b']['param'] = b\n",
    "    out = conv(x)\n",
    "    # The correct output\n",
    "    correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                               [-0.18387192, -0.2109216 ]],\n",
    "                              [[ 0.21027089,  0.21661097],\n",
    "                               [ 0.22847626,  0.23004637]],\n",
    "                              [[ 0.50813986,  0.54309974],\n",
    "                               [ 0.64082444,  0.67101435]]],\n",
    "                             [[[-0.98053589, -1.03143541],\n",
    "                               [-1.19128892, -1.24695841]],\n",
    "                              [[ 0.69108355,  0.66880383],\n",
    "                               [ 0.59480972,  0.56776003]],\n",
    "                              [[ 2.36270298,  2.36904306],\n",
    "                               [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "    print('Testing conv_forward')\n",
    "    print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pooling backward:\n",
      "dx error:  3.27562514223145e-12\n"
     ]
    }
   ],
   "source": [
    "if check_pool_back:\n",
    "    np.random.seed(231)\n",
    "    x = np.random.randn(3, 2, 8, 8)\n",
    "    dout = np.random.randn(3, 2, 4, 4)\n",
    "\n",
    "    pool = MaxPool(kernel_size=2, stride=2, padding=0)\n",
    "    out = pool(x)\n",
    "\n",
    "    dx = pool.backward(dout, x)\n",
    "\n",
    "    dx_num = eval_numerical_gradient_array(pool, x, dout)\n",
    "\n",
    "    # Your error should be around 1e-12\n",
    "    print('Testing pooling backward:')\n",
    "    print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv\n",
      "dx error:  1.5295788875965517e-08\n",
      "dw error:  8.309913739514208e-10\n",
      "db error:  2.4300919730707304e-11\n"
     ]
    }
   ],
   "source": [
    "# check convolutional backpropogation is correct:\n",
    "if check_conv_back:\n",
    "    x = np.random.randn(2, 3, 16, 16)\n",
    "    w = np.random.randn(3, 3, 3, 3)\n",
    "    b = np.random.randn(3, 1)\n",
    "    dout = np.random.randn(2, 3, 14, 14)\n",
    "    conv = Conv(in_channels=3, out_channels=3, height=3, width=3, stride=1, padding=0)\n",
    "    conv.params['b']['param'] = b\n",
    "    conv.params['w']['param'] = w # 此行为后添加的\n",
    "    out = conv(x)\n",
    "    dx = conv.backward(dout, x)\n",
    "\n",
    "    dx_num = eval_numerical_gradient_array(conv, x, dout)\n",
    "\n",
    "    params = conv.params\n",
    "\n",
    "\n",
    "    def fw(v):\n",
    "        tmp = params['w']['param']\n",
    "        params['w']['param'] = v\n",
    "        f_w = conv(x)\n",
    "        params['w']['param'] = tmp\n",
    "        return f_w\n",
    "\n",
    "\n",
    "    dw = params['w']['grad']\n",
    "    dw_num = eval_numerical_gradient_array(fw, w, dout)\n",
    "\n",
    "    db = params['b']['grad']\n",
    "\n",
    "\n",
    "    def fb(v):\n",
    "        tmp = params['b']['param']\n",
    "        params['b']['param'] = v\n",
    "        f_b = conv(x)\n",
    "        params['b']['param'] = tmp\n",
    "        return f_b\n",
    "\n",
    "\n",
    "    db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "    print('Testing conv')\n",
    "    print('dx error: ', rel_error(dx_num, dx))\n",
    "    print('dw error: ', rel_error(dw_num, dw))\n",
    "    print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consuming: 0.055562s\n",
      "loss: 2.3024449434898395\n",
      "w1 relative error: 4.104933e-07\n",
      "w2 relative error: 1.043347e-05\n",
      "w3 relative error: 7.761086e-06\n",
      "b1 relative error: 7.614735e-09\n",
      "b2 relative error: 3.194184e-08\n",
      "b3 relative error: 4.500063e-06\n"
     ]
    }
   ],
   "source": [
    "# TODO: write script to check the backpropagation on the whole CNN is correct\n",
    "if check_cnn_back:\n",
    "    inputs=2\n",
    "    input_dim=(3,16,16)\n",
    "    hidden_units=10\n",
    "    num_classes=10\n",
    "    num_filters=3\n",
    "    filter_size=3\n",
    "    pool_size=2\n",
    "\n",
    "    np.random.seed(231)\n",
    "    X = np.random.randn(inputs, *input_dim)\n",
    "    y = np.random.randint(num_classes, size=inputs)\n",
    "\n",
    "    model = CNN(input_dim, num_filters, filter_size, pool_size, hidden_units, num_classes)\n",
    "    start_time=time()\n",
    "    loss, score = model.oracle(X, y)\n",
    "    end_time=time()\n",
    "    print('Time consuming: %fs' % (end_time-start_time))\n",
    "    print('loss:', loss)\n",
    "    a=['w1','w2','w3']\n",
    "    b=['b1','b2','b3']\n",
    "\n",
    "    for param_name in sorted(a):\n",
    "        f = lambda _: model.oracle(X, y)[0]\n",
    "        param_grad_num = eval_numerical_gradient(f, model.param_groups['w'][param_name]['param'], verbose=False, h=0.00001)\n",
    "        e = rel_error(param_grad_num, model.param_groups['w'][param_name]['grad'])\n",
    "        print('%s relative error: %e' % (param_name, rel_error(param_grad_num, model.param_groups['w'][param_name]['grad'])))\n",
    "\n",
    "    for param_name in sorted(b):\n",
    "        f = lambda _: model.oracle(X, y)[0]\n",
    "        param_grad_num = eval_numerical_gradient(f, model.param_groups['b'][param_name]['param'], verbose=False, h=0.00001)\n",
    "        e = rel_error(param_grad_num, model.param_groups['b'][param_name]['grad'])\n",
    "        print('%s relative error: %e' % (param_name, rel_error(param_grad_num, model.param_groups['b'][param_name]['grad'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import numpy as np\n",
    "from nn.optimizer import SGD\n",
    "from nn.utils import accuracy\n",
    "from dataset import get_cifar10_data\n",
    "from nn.cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 100 , epoch 0) loss: 2.426176, train_accu: 0.000000\n",
      "(Iteration 2 / 100 , epoch 0) loss: 2.327081, train_accu: 0.080000\n",
      "(Iteration 3 / 100 , epoch 1) loss: 2.301251, train_accu: 0.080000\n",
      "(Iteration 4 / 100 , epoch 1) loss: 2.263435, train_accu: 0.140000\n",
      "(Iteration 5 / 100 , epoch 2) loss: 2.236490, train_accu: 0.240000\n",
      "(Iteration 6 / 100 , epoch 2) loss: 2.167822, train_accu: 0.240000\n",
      "(Iteration 7 / 100 , epoch 3) loss: 2.169647, train_accu: 0.280000\n",
      "(Iteration 8 / 100 , epoch 3) loss: 2.277372, train_accu: 0.100000\n",
      "(Iteration 9 / 100 , epoch 4) loss: 2.205201, train_accu: 0.180000\n",
      "(Iteration 10 / 100 , epoch 4) loss: 2.088595, train_accu: 0.380000\n",
      "(Iteration 11 / 100 , epoch 5) loss: 2.092491, train_accu: 0.260000\n",
      "(Iteration 12 / 100 , epoch 5) loss: 2.089308, train_accu: 0.300000\n",
      "(Iteration 13 / 100 , epoch 6) loss: 2.055046, train_accu: 0.300000\n",
      "(Iteration 14 / 100 , epoch 6) loss: 2.119532, train_accu: 0.220000\n",
      "(Iteration 15 / 100 , epoch 7) loss: 2.065627, train_accu: 0.260000\n",
      "(Iteration 16 / 100 , epoch 7) loss: 2.023578, train_accu: 0.340000\n",
      "(Iteration 17 / 100 , epoch 8) loss: 2.099163, train_accu: 0.320000\n",
      "(Iteration 18 / 100 , epoch 8) loss: 1.932777, train_accu: 0.400000\n",
      "(Iteration 19 / 100 , epoch 9) loss: 1.850631, train_accu: 0.520000\n",
      "(Iteration 20 / 100 , epoch 9) loss: 1.900760, train_accu: 0.380000\n",
      "(Iteration 21 / 100 , epoch 10) loss: 1.924896, train_accu: 0.340000\n",
      "(Iteration 22 / 100 , epoch 10) loss: 1.837094, train_accu: 0.460000\n",
      "(Iteration 23 / 100 , epoch 11) loss: 1.868651, train_accu: 0.380000\n",
      "(Iteration 24 / 100 , epoch 11) loss: 2.010926, train_accu: 0.260000\n",
      "(Iteration 25 / 100 , epoch 12) loss: 1.871535, train_accu: 0.360000\n",
      "(Iteration 26 / 100 , epoch 12) loss: 1.804405, train_accu: 0.500000\n",
      "(Iteration 27 / 100 , epoch 13) loss: 1.876917, train_accu: 0.360000\n",
      "(Iteration 28 / 100 , epoch 13) loss: 1.747030, train_accu: 0.620000\n",
      "(Iteration 29 / 100 , epoch 14) loss: 1.762065, train_accu: 0.520000\n",
      "(Iteration 30 / 100 , epoch 14) loss: 1.791088, train_accu: 0.480000\n",
      "(Iteration 31 / 100 , epoch 15) loss: 1.688922, train_accu: 0.560000\n",
      "(Iteration 32 / 100 , epoch 15) loss: 1.762341, train_accu: 0.500000\n",
      "(Iteration 33 / 100 , epoch 16) loss: 1.763246, train_accu: 0.560000\n",
      "(Iteration 34 / 100 , epoch 16) loss: 1.579958, train_accu: 0.560000\n",
      "(Iteration 35 / 100 , epoch 17) loss: 1.705783, train_accu: 0.440000\n",
      "(Iteration 36 / 100 , epoch 17) loss: 1.579553, train_accu: 0.500000\n",
      "(Iteration 37 / 100 , epoch 18) loss: 1.746652, train_accu: 0.460000\n",
      "(Iteration 38 / 100 , epoch 18) loss: 1.509119, train_accu: 0.600000\n",
      "(Iteration 39 / 100 , epoch 19) loss: 1.493257, train_accu: 0.500000\n",
      "(Iteration 40 / 100 , epoch 19) loss: 1.549597, train_accu: 0.520000\n",
      "(Iteration 41 / 100 , epoch 20) loss: 1.418657, train_accu: 0.740000\n",
      "(Iteration 42 / 100 , epoch 20) loss: 1.413299, train_accu: 0.540000\n",
      "(Iteration 43 / 100 , epoch 21) loss: 1.459923, train_accu: 0.520000\n",
      "(Iteration 44 / 100 , epoch 21) loss: 1.383961, train_accu: 0.600000\n",
      "(Iteration 45 / 100 , epoch 22) loss: 1.436357, train_accu: 0.600000\n",
      "(Iteration 46 / 100 , epoch 22) loss: 1.364429, train_accu: 0.660000\n",
      "(Iteration 47 / 100 , epoch 23) loss: 1.199187, train_accu: 0.760000\n",
      "(Iteration 48 / 100 , epoch 23) loss: 1.322678, train_accu: 0.740000\n",
      "(Iteration 49 / 100 , epoch 24) loss: 1.210083, train_accu: 0.740000\n",
      "(Iteration 50 / 100 , epoch 24) loss: 1.374967, train_accu: 0.680000\n",
      "(Iteration 51 / 100 , epoch 25) loss: 1.298556, train_accu: 0.660000\n",
      "(Iteration 52 / 100 , epoch 25) loss: 1.228099, train_accu: 0.700000\n",
      "(Iteration 53 / 100 , epoch 26) loss: 1.172449, train_accu: 0.720000\n",
      "(Iteration 54 / 100 , epoch 26) loss: 1.254064, train_accu: 0.660000\n",
      "(Iteration 55 / 100 , epoch 27) loss: 1.054541, train_accu: 0.800000\n",
      "(Iteration 56 / 100 , epoch 27) loss: 1.141000, train_accu: 0.660000\n",
      "(Iteration 57 / 100 , epoch 28) loss: 1.053096, train_accu: 0.840000\n",
      "(Iteration 58 / 100 , epoch 28) loss: 1.140016, train_accu: 0.700000\n",
      "(Iteration 59 / 100 , epoch 29) loss: 1.107669, train_accu: 0.800000\n",
      "(Iteration 60 / 100 , epoch 29) loss: 1.020527, train_accu: 0.800000\n",
      "(Iteration 61 / 100 , epoch 30) loss: 0.932400, train_accu: 0.900000\n",
      "(Iteration 62 / 100 , epoch 30) loss: 0.960570, train_accu: 0.800000\n",
      "(Iteration 63 / 100 , epoch 31) loss: 1.113666, train_accu: 0.800000\n",
      "(Iteration 64 / 100 , epoch 31) loss: 1.014728, train_accu: 0.780000\n",
      "(Iteration 65 / 100 , epoch 32) loss: 1.024536, train_accu: 0.800000\n",
      "(Iteration 66 / 100 , epoch 32) loss: 1.015848, train_accu: 0.820000\n",
      "(Iteration 67 / 100 , epoch 33) loss: 0.861031, train_accu: 0.820000\n",
      "(Iteration 68 / 100 , epoch 33) loss: 0.966859, train_accu: 0.720000\n",
      "(Iteration 69 / 100 , epoch 34) loss: 0.796964, train_accu: 0.880000\n",
      "(Iteration 70 / 100 , epoch 34) loss: 0.823801, train_accu: 0.880000\n",
      "(Iteration 71 / 100 , epoch 35) loss: 0.801481, train_accu: 0.820000\n",
      "(Iteration 72 / 100 , epoch 35) loss: 0.742113, train_accu: 0.940000\n",
      "(Iteration 73 / 100 , epoch 36) loss: 0.826643, train_accu: 0.800000\n",
      "(Iteration 74 / 100 , epoch 36) loss: 0.809369, train_accu: 0.880000\n",
      "(Iteration 75 / 100 , epoch 37) loss: 0.725861, train_accu: 0.880000\n",
      "(Iteration 76 / 100 , epoch 37) loss: 0.723860, train_accu: 0.920000\n",
      "(Iteration 77 / 100 , epoch 38) loss: 0.676312, train_accu: 0.840000\n",
      "(Iteration 78 / 100 , epoch 38) loss: 0.700067, train_accu: 0.920000\n",
      "(Iteration 79 / 100 , epoch 39) loss: 0.658046, train_accu: 0.900000\n",
      "(Iteration 80 / 100 , epoch 39) loss: 0.687082, train_accu: 0.940000\n",
      "(Iteration 81 / 100 , epoch 40) loss: 0.596588, train_accu: 0.920000\n",
      "(Iteration 82 / 100 , epoch 40) loss: 0.623103, train_accu: 0.900000\n",
      "(Iteration 83 / 100 , epoch 41) loss: 0.617530, train_accu: 0.920000\n",
      "(Iteration 84 / 100 , epoch 41) loss: 0.592684, train_accu: 0.920000\n",
      "(Iteration 85 / 100 , epoch 42) loss: 0.569720, train_accu: 0.960000\n",
      "(Iteration 86 / 100 , epoch 42) loss: 0.648132, train_accu: 0.880000\n",
      "(Iteration 87 / 100 , epoch 43) loss: 0.510650, train_accu: 1.000000\n",
      "(Iteration 88 / 100 , epoch 43) loss: 0.511765, train_accu: 0.940000\n",
      "(Iteration 89 / 100 , epoch 44) loss: 0.480551, train_accu: 0.960000\n",
      "(Iteration 90 / 100 , epoch 44) loss: 0.484914, train_accu: 0.960000\n",
      "(Iteration 91 / 100 , epoch 45) loss: 0.321901, train_accu: 0.980000\n",
      "(Iteration 92 / 100 , epoch 45) loss: 0.341340, train_accu: 1.000000\n",
      "(Iteration 93 / 100 , epoch 46) loss: 0.438660, train_accu: 0.980000\n",
      "(Iteration 94 / 100 , epoch 46) loss: 0.496042, train_accu: 0.900000\n",
      "(Iteration 95 / 100 , epoch 47) loss: 0.520762, train_accu: 0.920000\n",
      "(Iteration 96 / 100 , epoch 47) loss: 0.390176, train_accu: 1.000000\n",
      "(Iteration 97 / 100 , epoch 48) loss: 0.384273, train_accu: 0.980000\n",
      "(Iteration 98 / 100 , epoch 48) loss: 0.416097, train_accu: 0.960000\n",
      "(Iteration 99 / 100 , epoch 49) loss: 0.375839, train_accu: 0.940000\n",
      "(Iteration 100 / 100 , epoch 49) loss: 0.359517, train_accu: 0.960000\n"
     ]
    }
   ],
   "source": [
    "def train(model, X_train, y_train, X_val, y_val, batch_size, n_epochs, lr=1e-2,\n",
    "          lr_decay=0.8, verbose=True, print_level=100):\n",
    "    n_train = X_train.shape[0]\n",
    "    iterations_per_epoch = max(n_train // batch_size, 1)\n",
    "    n_iterations = n_epochs * iterations_per_epoch\n",
    "    epoch = 0\n",
    "    \n",
    "    loss_hist = []\n",
    "\n",
    "    # Define optimizer and set parameters\n",
    "    opt_params = {'lr': 1e-3}\n",
    "    sgd = SGD(model.param_groups, **opt_params)\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        batch_mask = np.random.choice(n_train, batch_size)\n",
    "        X_batch = X_val[batch_mask]\n",
    "        y_batch = y_val[batch_mask]        \n",
    "        # Evaluate function value and gradient\n",
    "        loss, score = model.oracle(X_batch, y_batch)\n",
    "        loss_hist.append(loss)\n",
    "\n",
    "        # Perform stochastic gradient descent\n",
    "\n",
    "        sgd.step()\n",
    "\n",
    "        # Maybe print training loss\n",
    "        if verbose and t % print_level == 0:\n",
    "            train_acc = accuracy(score, y_batch)\n",
    "            print('(Iteration %d / %d , epoch %d) loss: %f, train_accu: %f' % (\n",
    "                t + 1, n_iterations, epoch, loss_hist[-1], train_acc))\n",
    "\n",
    "        # At the end of every epoch, adjust the learning rate.\n",
    "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "        if epoch_end:\n",
    "            epoch += 1\n",
    "            opt_params['lr'] *= lr_decay\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = CNN()\n",
    "    data = get_cifar10_data()\n",
    "    num_train = 100\n",
    "    data = {\n",
    "        'X_train': data['X_train'][:num_train],\n",
    "        'y_train': data['y_train'][:num_train],\n",
    "        'X_val': data['X_val'],\n",
    "        'y_val': data['y_val'],\n",
    "    }\n",
    "    X_train, y_train, X_val, y_val = data['X_train'], data['y_train'], data['X_val'], data['y_val']\n",
    "\n",
    "    train(model, X_train, y_train, X_val, y_val, batch_size=50, n_epochs=50, print_level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it overfits on small data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accerleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from time import time\n",
    "from numba import jit\n",
    "from nn.loss import SoftmaxCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    \"\"\"Convolutional neural network with the following structures:\n",
    "        (conv + relu + pooling) + (linear + relu + linear) + softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size=(3, 32, 32), channels=32, conv_kernel=7,\n",
    "                 pool_kernel=2, hidden_units=100, n_classes=10):\n",
    "        \"\"\"\n",
    "        :param image_size: an 3 * H * W image, for color image,\n",
    "\n",
    "        :param channels: channels in the convolution layer\n",
    "        :param conv_kernel: kernel size of convolutional layer\n",
    "        :param pool_kernel: kernel size of pooling layer\n",
    "        :param hidden_units: number of hidden units in linear transform\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: initialize the neural network. Define the layers\n",
    "        C,H,W=image_size\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv = Conv(C,channels,conv_kernel,conv_kernel)\n",
    "        self.relu = Relu()\n",
    "        self.pool = MaxPool(pool_kernel)\n",
    "\n",
    "        # Fc layer\n",
    "        pad_out = 1 + (H - conv_kernel)\n",
    "        linear_in = (pad_out // 2 ) * (pad_out // 2 ) * channels\n",
    "        self.linear1 = Linear(linear_in, hidden_units)\n",
    "        self.linear2 = Linear(hidden_units, n_classes)\n",
    "        Scaling = self.linear1.init_scale\n",
    "\n",
    "        # TODO: Add the layers' parameters to the network, which will be assigned to optimizers\n",
    "        self.param_groups={\n",
    "            'w':{\n",
    "                'w1':{'param':Scaling * np.random.randn(channels, C, conv_kernel, conv_kernel),'grad':{}},\n",
    "                'w2':{'param':Scaling * np.random.randn(linear_in, hidden_units),'grad':{}},\n",
    "                'w3':{'param':Scaling * np.random.randn(hidden_units, n_classes),'grad':{}}\n",
    "            },\n",
    "            'b':{\n",
    "                'b1':{'param':np.zeros(channels),'grad':{}},\n",
    "                'b2':{'param':np.zeros(hidden_units),'grad':{}},\n",
    "                'b3':{'param':np.zeros(n_classes),'grad':{}}\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 forward time: 0.008931s\n",
      "conv1_relu forward time: 0.000022s\n",
      "conv1_relu_pool forward time: 0.002731s\n",
      "linear1 forward time: 0.000046s\n",
      "linear1_relu forward time: 0.000102s\n",
      "linear2 forward time: 0.000019s\n",
      "linear2 backward time: 0.000036s\n",
      "linear2_relu backward time: 0.000013s\n",
      "linear1 backward time: 0.000470s\n",
      "pooling backward time: 0.011582s\n",
      "conv_relu backward time: 0.000044s\n",
      "conv backward time: 0.022517s\n"
     ]
    }
   ],
   "source": [
    "if check_time:\n",
    "    inputs=2\n",
    "    input_dim=(3,16,16)\n",
    "    hidden_units=10\n",
    "    num_classes=10\n",
    "    num_filters=3\n",
    "    filter_size=3\n",
    "    pool_size=2\n",
    "\n",
    "    np.random.seed(231)\n",
    "    X = np.random.randn(inputs, *input_dim)\n",
    "    y = np.random.randint(num_classes, size=inputs)\n",
    "\n",
    "    model = CNN(input_dim, num_filters, filter_size, pool_size, hidden_units, num_classes)\n",
    "    \n",
    "    model.conv.params['w']['param']=model.param_groups['w']['w1']['param']\n",
    "    model.conv.params['b']['param']=model.param_groups['b']['b1']['param']\n",
    "    model.linear1.params['w']['param']=model.param_groups['w']['w2']['param']\n",
    "    model.linear1.params['b']['param']=model.param_groups['b']['b2']['param']\n",
    "    model.linear2.params['w']['param']=model.param_groups['w']['w3']['param']\n",
    "    model.linear2.params['b']['param']=model.param_groups['b']['b3']['param']\n",
    "\n",
    "    start=time()\n",
    "    conv1=model.conv.forward(x)\n",
    "    end=time()\n",
    "    print('conv1 forward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    conv1_relu=model.relu.forward(conv1)\n",
    "    end=time()\n",
    "    print('conv1_relu forward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    conv1_relu_pool=model.pool.forward(conv1_relu)\n",
    "    end=time()\n",
    "    print('conv1_relu_pool forward time: %fs' % (end-start))\n",
    "\n",
    "    # FC-layer2\n",
    "    start=time()\n",
    "    linear1=model.linear1.forward(conv1_relu_pool)\n",
    "    end=time()\n",
    "    print('linear1 forward time: %fs' %(end-start))\n",
    "\n",
    "    start=time()\n",
    "    linear1_relu=model.relu.forward(linear1)\n",
    "    end=time()\n",
    "    print('linear1_relu forward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    linear2=model.linear2.forward(linear1_relu)\n",
    "    end=time()\n",
    "    print('linear2 forward time: %fs' % (end-start)) \n",
    "    \n",
    "    s = linear2\n",
    "    fx,dloss=SoftmaxCE.__call__(s,y)\n",
    "        \n",
    "    start=time()\n",
    "    dlinear2= model.linear2.backward(dloss,linear1_relu)\n",
    "    end=time()\n",
    "    print('linear2 backward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    dlinear2_relu=model.relu.backward(dlinear2,linear1)\n",
    "    end=time()\n",
    "    print('linear2_relu backward time: %fs' % (end-start))\n",
    "        \n",
    "    start=time()\n",
    "    dlinear1=model.linear1.backward(dlinear2_relu,conv1_relu_pool)\n",
    "    end=time()\n",
    "    print('linear1 backward time: %fs' % (end-start))\n",
    "\n",
    "    # FC-layer2\n",
    "    start=time()\n",
    "    dpool = model.pool.backward(dlinear1,conv1_relu)\n",
    "    end=time()\n",
    "    print('pooling backward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    dr = model.relu.backward(dpool,conv1)\n",
    "    end=time()\n",
    "    print('conv_relu backward time: %fs' % (end-start))\n",
    "\n",
    "    start=time()\n",
    "    dconv= model.conv.backward(dr,x)\n",
    "    end=time()\n",
    "    print('conv backward time: %fs' % (end-start)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**挑选了时间量级较大的conv_forward,conv_backward,pool_forward,pool_backward适用jit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过验证发现jit在简单运算上反而变慢了，但是如果带入train进行迭代，速度就变快了很多，大概能节省将近一半的时间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
