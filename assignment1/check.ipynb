{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(wd, n_hidden, n_iters, learning_rate, momentum_mul, do_early_stopping=False, minibatch_size=10):\n",
    "    \"\"\"\n",
    "    a simple multilayer neural net for multiclass classification\n",
    "    :param wd: weight_decay \n",
    "    :param n_hidden: number of units in hidden layer\n",
    "    :param n_iters: number of sgd iteration\n",
    "    :param learning_rate: \n",
    "    :param momentum_mul: velocity damping factor\n",
    "    :param do_early_stopping: True if early_stopping is done, where we simply report the best past solution\n",
    "    :param minibatch_size: size of minibatch in sgd\n",
    "    :return: classification loss on the datasets\n",
    "    \"\"\"\n",
    "    data_file = loadmat('data.mat', squeeze_me=True, struct_as_record=False)\n",
    "    data = data_file['data']\n",
    "\n",
    "    data_train = {'X': data.training.inputs, 'y': data.training.targets}\n",
    "    data_valid = {'X': data.validation.inputs, 'y': data.validation.targets}\n",
    "    data_test = {'X': data.test.inputs, 'y': data.test.targets}\n",
    "    n_train = data_train['X'].shape[1]\n",
    "\n",
    "    # initialize model\n",
    "    params = initial_model(n_hidden)\n",
    "    theta = model2theta(params)\n",
    "\n",
    "    test_gradient(params, data_train, wd)\n",
    "    \n",
    "    # initialize velocity\n",
    "    v = 0\n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    best = {}\n",
    "\n",
    "    if do_early_stopping:\n",
    "        best['theta'] = 0\n",
    "        best['loss_valid'] = np.inf\n",
    "        best['iter'] = -1\n",
    "\n",
    "    for t in range(n_iters + 1):\n",
    "        batch_start = (t * minibatch_size) % n_train\n",
    "        data_batch = {\n",
    "            'X': data_train['X'][:, batch_start:batch_start + minibatch_size],\n",
    "            'y': data_train['y'][:, batch_start:batch_start + minibatch_size]\n",
    "        }\n",
    "\n",
    "        # classical momentum\n",
    "        loss, grad = eval_obj_grad(theta2model(theta), data_batch, wd)\n",
    "        grad_vec = model2theta(grad)\n",
    "        v = momentum_mul * v - grad_vec\n",
    "        theta += learning_rate * v\n",
    "\n",
    "        # todo Nesterov's accelerated method\n",
    "        #theta_nestoerov = theta + momentum_mul * v \n",
    "        #loss, grad = eval_obj_grad(theta2model(theta_nestoerov), data_batch, wd)\n",
    "        #grad_vec = model2theta(grad)\n",
    "        #v = momentum_mul * v - grad_vec\n",
    "        #theta += learning_rate * v\n",
    "\n",
    "        loss = eval_obj(params, data_train, wd)\n",
    "        loss_train.append(loss)\n",
    "        loss = eval_obj(params, data_valid, wd)\n",
    "        loss_valid.append(loss)\n",
    "\n",
    "        if do_early_stopping and loss_valid[-1] < best['loss_valid']:\n",
    "            best['theta'] = theta.copy()\n",
    "            best['loss_valid'] = loss_valid[-1]\n",
    "            best['iter'] = t\n",
    "\n",
    "        if t % (max(1, n_iters // 10)) == 0:\n",
    "            print('After %d iterations, ||theta|| %.3e, training loss is %.2e, and validation loss is %.2e\\n' % (\n",
    "            t, norm(theta), loss_train[-1],loss_valid[-1]))\n",
    "\n",
    "    test_gradient(params, data_train, wd)\n",
    "\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(loss_train, label='training loss')\n",
    "    plt.plot(loss_valid, label='validation loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    if do_early_stopping:\n",
    "        print(\"Early stopping: validation loss: %.3e,  was lowest after %d iterations\" % (\n",
    "        best['loss_valid'], best['iter']))\n",
    "        theta = best['theta']\n",
    "\n",
    "    params = theta2model(theta)\n",
    "    # examine performance\n",
    "    datasets = [data_train, data_valid, data_test]\n",
    "\n",
    "    acc = [accuracy(params, x) for x in datasets]\n",
    "\n",
    "    classification_loss = [eval_obj(params, x, 0) for x in datasets]\n",
    "\n",
    "    print(\"Accuracy: training %.3e, validation %.3e, testing %.3e\" % (acc[0], acc[1], acc[2]))\n",
    "    info = {\n",
    "        'loss_train': classification_loss[0],\n",
    "        'loss_valid': classification_loss[1],\n",
    "        'loss_test': classification_loss[2]\n",
    "    }\n",
    "    return info\n",
    "\n",
    "\n",
    "def eval_obj(params, data, wd):\n",
    "    # W_hid, b_hid, W_out, b_out = params['W_hid'], params['b_hid'], params['W_out'], params['b_out']\n",
    "    W_hid, W_out = params['W_hid'], params['W_out']\n",
    "    # todo implement the forward propagation\n",
    "    z_hid = W_hid.dot(data['X'])\n",
    "    a_hid = sigmoid(z_hid)\n",
    "    z_out = W_out.dot(a_hid)\n",
    "    y_predict = np.exp(z_out-log_sum_exp(z_out))\n",
    "\n",
    "    loss = 0\n",
    "    cross_entropy = -np.log(y_predict)*data['y']\n",
    "    weight_decay_loss=1/2*(np.square(np.linalg.norm(W_out))+np.square(np.linalg.norm(W_hid)))\n",
    "    loss = np.sum(cross_entropy)/data['X'].shape[1] + wd*weight_decay_loss/data['y'].shape[1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_obj_grad(params, data, wd):\n",
    "    \"\"\"\n",
    "    compute loss and gradient of model\n",
    "    :param params: \n",
    "                    W_hid\n",
    "                    W_out\n",
    "    :param data:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    W_hid, W_out = params['W_hid'], params['W_out']\n",
    "    W_hid = np.array(W_hid)\n",
    "    W_out = np.array(W_out)\n",
    "\n",
    "    # todo implement the forward propagation\n",
    "    z_hid = W_hid.dot(data['X'])\n",
    "    a_hid = sigmoid(z_hid)\n",
    "    z_out = W_out.dot(a_hid)\n",
    "    y_predict = np.exp(z_out-log_sum_exp(z_out))\n",
    "\n",
    "    loss = 0\n",
    "    cross_entropy = -np.log(y_predict)*data['y']\n",
    "    weight_decay_loss=1/2*(np.square(np.linalg.norm(W_out))+np.square(np.linalg.norm(W_hid)))\n",
    "    loss = np.sum(cross_entropy)/data['X'].shape[1] + wd*weight_decay_loss/data['y'].shape[1]\n",
    "\n",
    "    # todo implement the backward prapagation\n",
    "    n_hidden = 100\n",
    "    grad_W_out = np.zeros([10, n_hidden])\n",
    "    grad_W_hid = np.zeros([n_hidden, 256])\n",
    "    \n",
    "    # computing the output gradient\n",
    "    delta_out = (y_predict - data['y']) / data['y'].shape[1]\n",
    "    grad_W_out = np.array(np.dot(delta_out, np.transpose(a_hid)) + wd * W_out / data['X'].shape[1])\n",
    "    \n",
    "    # computing the hidden gradient\n",
    "    delta_hid  = a_hid * (1 - a_hid) * np.dot(np.transpose(W_out), delta_out)      \n",
    "    grad_W_hid = np.array(np.dot(delta_hid, np.transpose(data['X'])) + wd * W_hid / data['X'].shape[1])\n",
    "    \n",
    "    grad = {'W_out': grad_W_out,\n",
    "            'W_hid': grad_W_hid,\n",
    "           }\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "def initial_model(n_hid):\n",
    "    n_params = (256 + 10) * n_hid\n",
    "    as_row_vector = np.cos(np.arange(n_params))\n",
    "    params = {}\n",
    "    params['W_hid'] = as_row_vector[:256 * n_hid].reshape((n_hid, 256)) * 0.1\n",
    "    params['W_out'] = as_row_vector[256 * n_hid:].reshape((10, n_hid)) * 0.1\n",
    "    return params\n",
    "\n",
    "\n",
    "def test_gradient(params, data, wd):\n",
    "    loss, analytic_grad = eval_obj_grad(params, data, wd)\n",
    "\n",
    "    num_checks = 100\n",
    "    theta = model2theta(params)\n",
    "    grad_ana = model2theta(analytic_grad)\n",
    "\n",
    "    delta = 1e-4\n",
    "    threshold = 1e-5\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ind = (i * 1299283) % theta.size\n",
    "        grad_ind_ana = grad_ana[ind]\n",
    "\n",
    "        theta1 = theta.copy()\n",
    "        theta1[ind] += delta\n",
    "        l1 = eval_obj(theta2model(theta1), data, wd)\n",
    "\n",
    "        theta2 = theta.copy()\n",
    "        theta2[ind] -= delta\n",
    "        l2 = eval_obj(theta2model(theta2), data, wd)\n",
    "\n",
    "        grad_ind_fin = (l1 - l2) / (2 * delta)\n",
    "        diff = abs(grad_ind_ana - grad_ind_fin)\n",
    "        if diff < threshold:\n",
    "            continue\n",
    "        if diff / (abs(grad_ind_ana) + abs(grad_ind_fin)) < threshold:\n",
    "            continue\n",
    "        raise AssertionError('%d-th: l %.3e, l1 %.3e, l2 %.3e, analytic %.3e, fd %.3e, diff %.3e\\n'\n",
    "                             % (i, loss, l1, l2, grad_ind_ana, grad_ind_fin, diff))\n",
    "    print('Gradient test passed')\n",
    "\n",
    "\n",
    "def model2theta(params):\n",
    "    \"\"\"\n",
    "    convert model parameters into vector form\n",
    "    :param params: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    theta = np.concatenate((params['W_out'].flatten(), params['W_hid'].flatten()))\n",
    "    return theta\n",
    "\n",
    "\n",
    "def theta2model(theta):\n",
    "    \"\"\"\n",
    "    convert vector form into model parameters\n",
    "    :param theta: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    n_hid = theta.size // (256 + 10)\n",
    "    params = {}\n",
    "    params['W_out'] = np.reshape(theta[:n_hid * 10], (10, n_hid))\n",
    "    params['W_hid'] = np.reshape(theta[n_hid * 10:], (n_hid, 256))\n",
    "    return params\n",
    "\n",
    "\n",
    "def accuracy(params, data):\n",
    "    W_hid, W_out = params['W_hid'], params['W_out']\n",
    "\n",
    "    # indices of class label\n",
    "    # class_indices = np.nonzero(data['y'])\n",
    "    index_transpose = np.nonzero(data['y'].T)\n",
    "    true_label = index_transpose[1]\n",
    "    # forward propagation\n",
    "    a_hidden = W_hid.dot(data['X'])\n",
    "    h_hidden = sigmoid(a_hidden)\n",
    "\n",
    "    a_out = W_out.dot(h_hidden)\n",
    "\n",
    "    pred = a_out.argmax(axis=0)\n",
    "\n",
    "    return np.mean(pred == true_label)\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\"\n",
    "    compute log(sum(exp(a), 0)), should return a n-dim vector\n",
    "    :param x: p*n matrix\n",
    "    \"\"\"\n",
    "    # todo implement the log column sum of exp(x)\n",
    "    lse_x= np.log(np.sum(np.exp(x), axis = 0))\n",
    "    \n",
    "    return lse_x\n",
    "\n",
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=10\n",
    "wd=0\n",
    "do_early_stopping = False\n",
    "n_iters=70\n",
    "minibatch_size=10\n",
    "momentum_mul=0\n",
    "learning_rate=0.05\n",
    "\n",
    "data_file = loadmat('data.mat', squeeze_me=True, struct_as_record=False)\n",
    "data = data_file['data']\n",
    "\n",
    "data_train = {'X': data.training.inputs, 'y': data.training.targets}\n",
    "data_valid = {'X': data.validation.inputs, 'y': data.validation.targets}\n",
    "data_test = {'X': data.test.inputs, 'y': data.test.targets}\n",
    "n_train = data_train['X'].shape[1]\n",
    "    \n",
    "# initialize model\n",
    "params = initial_model(n_hidden)\n",
    "theta = model2theta(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient test passed\n"
     ]
    }
   ],
   "source": [
    "test_gradient(params,data_train,wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 iterations, ||theta|| 3.647e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 7 iterations, ||theta|| 3.647e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 14 iterations, ||theta|| 3.647e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 21 iterations, ||theta|| 3.647e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 28 iterations, ||theta|| 3.648e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 35 iterations, ||theta|| 3.648e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 42 iterations, ||theta|| 3.649e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 49 iterations, ||theta|| 3.650e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 56 iterations, ||theta|| 3.652e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 63 iterations, ||theta|| 3.653e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "After 70 iterations, ||theta|| 3.655e+00, training loss is 2.30e+00, and validation loss is 2.30e+00\n",
      "\n",
      "Gradient test passed\n"
     ]
    }
   ],
   "source": [
    "    # initialize velocity\n",
    "    v = 0\n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    best = {}\n",
    "\n",
    "    if do_early_stopping:\n",
    "        best['theta'] = 0\n",
    "        best['loss_valid'] = np.inf\n",
    "        best['iter'] = -1\n",
    "\n",
    "    for t in range(n_iters + 1):\n",
    "        batch_start = (t * minibatch_size) % n_train\n",
    "        data_batch = {\n",
    "            'X': data_train['X'][:, batch_start:batch_start + minibatch_size],\n",
    "            'y': data_train['y'][:, batch_start:batch_start + minibatch_size]\n",
    "        }\n",
    "\n",
    "        # classical momentum\n",
    "        loss, grad = eval_obj_grad(theta2model(theta), data_batch, wd)\n",
    "        grad_vec = model2theta(grad)\n",
    "        v = momentum_mul * v - grad_vec\n",
    "        theta += learning_rate * v\n",
    "\n",
    "        # todo Nesterov's accelerated method\n",
    "        #theta_nestoerov = theta + momentum_mul * v \n",
    "        #loss, grad = eval_obj_grad(theta2model(theta_nestoerov), data_batch, wd)\n",
    "        #grad_vec = model2theta(grad)\n",
    "        #v = momentum_mul * v - grad_vec\n",
    "        #theta += learning_rate * v\n",
    "\n",
    "        loss = eval_obj(params, data_train, wd)\n",
    "        loss_train.append(loss)\n",
    "        loss = eval_obj(params, data_valid, wd)\n",
    "        loss_valid.append(loss)\n",
    "\n",
    "        if do_early_stopping and loss_valid[-1] < best['loss_valid']:\n",
    "            best['theta'] = theta.copy()\n",
    "            best['loss_valid'] = loss_valid[-1]\n",
    "            best['iter'] = t\n",
    "\n",
    "        if t % (max(1, n_iters // 10)) == 0:\n",
    "            print('After %d iterations, ||theta|| %.3e, training loss is %.2e, and validation loss is %.2e\\n' % (\n",
    "            t, norm(theta), loss_train[-1],loss_valid[-1]))\n",
    "\n",
    "    test_gradient(params, data_train, wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.304867088252495"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
